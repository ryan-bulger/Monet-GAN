{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d713586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 1\n",
    "CHANNELS = 3\n",
    "# MAX_PHOTOS = 3\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "random.seed(666)\n",
    "\n",
    "# Data Loading\n",
    "photo_ds_path = './photo_jpg'\n",
    "monet_ds_path = './monet_jpg'\n",
    "\n",
    "photo_images = sorted(glob.glob(os.path.join(photo_ds_path, '*.jpg')))\n",
    "monet_images = sorted(glob.glob(os.path.join(monet_ds_path, '*.jpg')))\n",
    "\n",
    "# if len(photo_images) > MAX_PHOTOS:\n",
    "#     photo_images = sorted(random.sample(photo_images, MAX_PHOTOS))\n",
    "#     print(f'Randomly sampled {len(photo_images)} photos for training')\n",
    "\n",
    "comparison_photo_paths = random.sample(photo_images, min(10, len(photo_images)))\n",
    "epoch_snapshots = {}\n",
    "epoch_loss_history = {\"generator\": [], \"discriminator\": []}\n",
    "\n",
    "monet_files_tensor = tf.constant(monet_images)\n",
    "monet_count = tf.cast(tf.shape(monet_files_tensor)[0], tf.int64)\n",
    "\n",
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=CHANNELS)\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = (tf.cast(img, tf.float32) / 127.5) - 1.0\n",
    "    return img\n",
    "\n",
    "def denormalize_image(img):\n",
    "    img = (img + 1.0) / 2.0\n",
    "    return tf.clip_by_value(img, 0.0, 1.0)\n",
    "\n",
    "def map_pair(idx, photo_path):\n",
    "    monet_idx = tf.math.floormod(idx, monet_count)\n",
    "    monet_path = tf.gather(monet_files_tensor, monet_idx)\n",
    "    photo_img = load_image(photo_path)\n",
    "    monet_img = load_image(monet_path)\n",
    "    return photo_img, monet_img\n",
    "\n",
    "def generate_monet_batch(photo_paths):\n",
    "    outputs = []\n",
    "    for path in photo_paths:\n",
    "        photo = load_image(path)\n",
    "        monet_pred = monet_generator(tf.expand_dims(photo, axis=0), training=False)\n",
    "        monet_img = tf.squeeze(monet_pred, axis=0)\n",
    "        outputs.append(denormalize_image(monet_img).numpy())\n",
    "    return outputs\n",
    "\n",
    "def show_photo_to_monet(photo_paths=None, num_examples=3, monet_images=None, title='Original Image to Monet style Image'):\n",
    "    selected_paths = list(photo_paths)\n",
    "    sample_count = len(selected_paths)\n",
    "    fig, axes = plt.subplots(sample_count, 2, figsize=(5, 2.5 * sample_count))\n",
    "    if sample_count == 1:\n",
    "        axes = [axes]\n",
    "    for row_idx, path in enumerate(selected_paths):\n",
    "        row_axes = axes[row_idx]\n",
    "        photo_disp = denormalize_image(load_image(path)).numpy()\n",
    "        monet_disp = monet_images[row_idx]\n",
    "        row_axes[0].imshow(photo_disp)\n",
    "        row_axes[0].set_title('Original Photo')\n",
    "        row_axes[0].axis('off')\n",
    "        row_axes[1].imshow(monet_disp)\n",
    "        row_axes[1].set_title('Monet-style Output')\n",
    "        row_axes[1].axis('off')\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.985))\n",
    "    plt.show()\n",
    "\n",
    "def show_epoch_progression(photo_paths, snapshots):\n",
    "    epochs = sorted(snapshots.keys())\n",
    "    sample_count = len(photo_paths)\n",
    "    fig, axes = plt.subplots(sample_count, len(epochs) + 1, figsize=(4 * (len(epochs) + 1), 4 * sample_count))\n",
    "    if sample_count == 1:\n",
    "        axes = [axes]\n",
    "    original_images = [denormalize_image(load_image(path)).numpy() for path in photo_paths]\n",
    "    for row_idx, photo_img in enumerate(original_images):\n",
    "        row_axes = axes[row_idx]\n",
    "        row_axes[0].imshow(photo_img)\n",
    "        row_axes[0].set_title('Original Photo')\n",
    "        row_axes[0].axis('off')\n",
    "        for col_idx, epoch in enumerate(epochs, start=1):\n",
    "            ax = row_axes[col_idx]\n",
    "            ax.imshow(snapshots[epoch][row_idx])\n",
    "            ax.set_title(f'Epoch {epoch}')\n",
    "            ax.axis('off')\n",
    "    fig.suptitle('Original vs. Monet-style progression', fontsize=14)\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.97))\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    gen_losses = loss_history[\"generator\"]\n",
    "    disc_losses = loss_history[\"discriminator\"]\n",
    "    epochs = range(1, len(gen_losses) + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(epochs, gen_losses, label='Generator Loss')\n",
    "    plt.plot(epochs, disc_losses, label='Discriminator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Generator vs Discriminator Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(photo_images)\n",
    "dataset = dataset.shuffle(buffer_size=len(photo_images), reshuffle_each_iteration=True)\n",
    "dataset = dataset.enumerate()\n",
    "dataset = dataset.map(map_pair, num_parallel_calls=AUTOTUNE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "# Model Definitions\n",
    "def build_generator():\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "    x = tf.keras.layers.Conv2D(64, 7, padding='same')(inputs)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(256, 3, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    outputs = tf.keras.layers.Conv2D(3, 7, padding='same', activation='tanh')(x)\n",
    "    return tf.keras.Model(inputs, outputs, name='generator')\n",
    "\n",
    "def build_discriminator():\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    outputs = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "    return tf.keras.Model(inputs, outputs, name='discriminator')\n",
    "\n",
    "monet_generator = build_generator()\n",
    "photo_generator = build_generator()\n",
    "monet_discriminator = build_discriminator()\n",
    "photo_discriminator = build_discriminator()\n",
    "\n",
    "# Loss Functions and Optimizers\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer_G = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "optimizer_D = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "gen_vars = monet_generator.trainable_variables + photo_generator.trainable_variables\n",
    "disc_vars = monet_discriminator.trainable_variables + photo_discriminator.trainable_variables\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_photo, real_monet):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_monet = monet_generator(real_photo, training=True)\n",
    "        fake_photo = photo_generator(real_monet, training=True)\n",
    "\n",
    "        monet_real_logits = monet_discriminator(real_monet, training=True)\n",
    "        monet_fake_logits = monet_discriminator(fake_monet, training=True)\n",
    "        photo_real_logits = photo_discriminator(real_photo, training=True)\n",
    "        photo_fake_logits = photo_discriminator(fake_photo, training=True)\n",
    "\n",
    "        valid = tf.ones_like(monet_real_logits)\n",
    "        fake = tf.zeros_like(monet_fake_logits)\n",
    "\n",
    "        loss_G = mse(valid, monet_fake_logits) + mse(valid, photo_fake_logits)\n",
    "        loss_D_monet = mse(valid, monet_real_logits) + mse(fake, monet_fake_logits)\n",
    "        loss_D_photo = mse(valid, photo_real_logits) + mse(fake, photo_fake_logits)\n",
    "        loss_D = loss_D_monet + loss_D_photo\n",
    "\n",
    "    gen_grads = tape.gradient(loss_G, gen_vars)\n",
    "    disc_grads = tape.gradient(loss_D, disc_vars)\n",
    "\n",
    "    optimizer_G.apply_gradients(zip(gen_grads, gen_vars))\n",
    "    optimizer_D.apply_gradients(zip(disc_grads, disc_vars))\n",
    "\n",
    "    return loss_G, loss_D\n",
    "\n",
    "# Training Loop\n",
    "def train(dataset, epochs, visualize_interval):\n",
    "    global epoch_snapshots, epoch_loss_history\n",
    "\n",
    "    steps_per_epoch = len(photo_images) // BATCH_SIZE\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        progbar = tf.keras.utils.Progbar(target=steps_per_epoch, verbose=1)\n",
    "        for step, (real_photo, real_monet) in enumerate(dataset.take(steps_per_epoch), start=1):\n",
    "            loss_G, loss_D = train_step(real_photo, real_monet)\n",
    "            progbar.update(step, values=[('loss_G', float(loss_G)), ('loss_D', float(loss_D))])\n",
    "        print(f\"Epoch {epoch + 1} complete - loss_G: {float(loss_G):.3f}, loss_D: {float(loss_D):.3f}\")\n",
    "\n",
    "        epoch_loss_history[\"generator\"].append(float(loss_G))\n",
    "        epoch_loss_history[\"discriminator\"].append(float(loss_D))\n",
    "\n",
    "        if (epoch + 1) % visualize_interval == 0:\n",
    "            monet_outputs = generate_monet_batch(comparison_photo_paths)\n",
    "            epoch_snapshots[epoch + 1] = monet_outputs\n",
    "            show_photo_to_monet(photo_paths=comparison_photo_paths, monet_images=monet_outputs, title=f'Epoch {epoch + 1} progress')\n",
    "\n",
    "# Train model\n",
    "train(dataset, epochs=1000, visualize_interval=100)\n",
    "\n",
    "plot_loss_history(epoch_loss_history)\n",
    "\n",
    "# Visualize progression over the recorded epochs\n",
    "show_epoch_progression(comparison_photo_paths, epoch_snapshots)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
