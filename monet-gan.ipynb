{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ad186c",
   "metadata": {},
   "source": [
    "# Monet GAN Project\n",
    "\n",
    "Intro here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4bc754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 22:42:27.369863: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-13 22:42:27.406390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-13 22:42:28.226969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8327b",
   "metadata": {},
   "source": [
    "Tensorflow Hub  \n",
    "https://www.tensorflow.org/tutorials/generative/style_transfer#fast_style_transfer_using_tf-hub  \n",
    "https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0db7e",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55b23f",
   "metadata": {},
   "source": [
    "#### Data Source\n",
    "- Data was imported from the 'I’m Something of a Painter Myself' Kaggle competition.\n",
    "- 2 folders provided from Kaggle: monet_jpg and photo_jpg. These will be converted to Tensorflow record files so it's easier to do work with\n",
    "\n",
    "Reference:  \n",
    "Amy Jang, Ana Sofia Uzsoy, and Phil Culliton. I’m Something of a Painter Myself. https://kaggle.com/competitions/gan-getting-started, 2020. Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fecac",
   "metadata": {},
   "source": [
    "#### Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff92663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Monet paintings = 300\n",
      "Count of photo images = 7,038\n"
     ]
    }
   ],
   "source": [
    "# Paths to data folders and labels file\n",
    "monet_images_path = './monet_jpg'\n",
    "photos_images_path  = './photo_jpg'\n",
    "\n",
    "# Create a list of the file paths to the images in the monet and photos folders\n",
    "monet_file_paths = glob.glob(os.path.join(monet_images_path, '*.jpg'))\n",
    "photos_file_paths = glob.glob(os.path.join(photos_images_path, '*.jpg'))\n",
    "\n",
    "print(f'Count of Monet paintings = {len(monet_file_paths):,}\\nCount of photo images = {len(photos_file_paths):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5a4f3",
   "metadata": {},
   "source": [
    "#### Configure Constant Variables\n",
    "- Define the exogenous varibles that will be used throughout the modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5f0015b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(666)\n",
    "np.random.seed(666)\n",
    "\n",
    "# # Image config\n",
    "# IMG_SIZE = 96 # Target size to resize images to\n",
    "# CHANNELS = 3  # RGB images\n",
    "# SAMPLE_SIZE = 55000 # Number of image samples used for training\n",
    "\n",
    "# # Training config\n",
    "# BATCH_SIZE = 64 # Training batch size\n",
    "# AUTOTUNE = tf.data.AUTOTUNE # Let tf.data choose parallelism\n",
    "# MAX_EPOCHS = 30\n",
    "\n",
    "# Ignore warnings\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098108c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d457e3d",
   "metadata": {},
   "source": [
    "#### Similar Looking Pictures  \n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648734de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m matches_sorted\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Run matching for 300 pairs and display the top 10 correlated pairs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m matches = \u001b[43mfind_best_unique_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/ryan-bulger/Monet-GAN/monet_jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/ryan-bulger/Monet-GAN/photo_jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_matches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matches)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m matched pairs. Showing top 10 by correlation...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Display top 10 side-by-side\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mfind_best_unique_matches\u001b[39m\u001b[34m(monet_dir, photo_dir, n_matches)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Find up to n_matches unique photo matches for Monet images.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03mStrategy: precompute histograms for both sets, build a score matrix, then do a greedy unique assignment:\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03mprefer monet images with highest best-possible score first to reduce collisions.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# gather file lists (jpg only) and limit monet images to n_matches\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m monet_images = \u001b[38;5;28msorted\u001b[39m([os.path.join(monet_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m.listdir(monet_dir) \u001b[38;5;28;01mif\u001b[39;00m f.lower().endswith(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m)])[:n_matches]\n\u001b[32m     24\u001b[39m photo_images = \u001b[38;5;28msorted\u001b[39m([os.path.join(photo_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(photo_dir) \u001b[38;5;28;01mif\u001b[39;00m f.lower().endswith(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m)])\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(monet_images) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(photo_images) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute per-channel RGB histograms and find best unique matches between Monet and Photo sets\n",
    "def compute_rgb_hist(image_bgr, bins=256):\n",
    "    \"\"\"Return list of 3 normalized histograms for B,G,R channels.\"\"\"\n",
    "    hists = []\n",
    "    for ch in range(3):  # 0=B, 1=G, 2=R\n",
    "        hist = cv2.calcHist([image_bgr], [ch], None, [bins], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "        hists.append(hist)\n",
    "    return hists\n",
    "\n",
    "def channel_correlation(hist1, hist2):\n",
    "    \"\"\"Compare two single-channel histograms using correlation.\"\"\"\n",
    "    return cv2.compareHist(hist1.astype('float32'), hist2.astype('float32'), cv2.HISTCMP_CORREL)\n",
    "\n",
    "def find_best_unique_matches(monet_dir, photo_dir, n_matches=300):\n",
    "    # gather file lists (jpg only) and limit monet images to n_matches\n",
    "    monet_images = sorted([os.path.join(monet_dir, f) for f in os.listdir(monet_dir) if f.lower().endswith('.jpg')])[:n_matches]\n",
    "    photo_images = sorted([os.path.join(photo_dir, f) for f in os.listdir(photo_dir) if f.lower().endswith('.jpg')])\n",
    "\n",
    "    if len(monet_images) == 0 or len(photo_images) == 0:\n",
    "        raise ValueError('No images found in one of the folders')\n",
    "\n",
    "    # precompute photo histograms once\n",
    "    photo_hists = []\n",
    "    for p in photo_images:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "        photo_hists.append((p, compute_rgb_hist(img)))\n",
    "\n",
    "    # precompute monet histograms\n",
    "    monet_hists = []\n",
    "    for m in monet_images:\n",
    "        img = cv2.imread(m)\n",
    "        if img is None:\n",
    "            continue\n",
    "        monet_hists.append((m, compute_rgb_hist(img)))\n",
    "\n",
    "    # build score matrix (monet x photo)\n",
    "    scores = []\n",
    "    for (m_path, m_hist) in monet_hists:\n",
    "        row = []\n",
    "        for (p_path, p_hist) in photo_hists:\n",
    "            ch_scores = [channel_correlation(m_hist[i], p_hist[i]) for i in range(3)]\n",
    "            row.append(float(np.mean(ch_scores)))\n",
    "        scores.append(row)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # greedy unique assignment: order monet images by their best possible match (desc), then pick highest-scoring unassigned photo\n",
    "    monet_best_scores = scores.max(axis=1)\n",
    "    monet_order = np.argsort(-monet_best_scores)\n",
    "    assigned_photos = set()\n",
    "    matches = []\n",
    "    for mi in monet_order:\n",
    "        photo_ranking = np.argsort(-scores[mi])  # indices sorted by descending score\n",
    "        chosen = None\n",
    "        for pj in photo_ranking:\n",
    "            if pj not in assigned_photos:\n",
    "                chosen = pj\n",
    "                break\n",
    "        # if all photos already assigned (rare if more photos than monet), allow reuse of best\n",
    "        if chosen is None:\n",
    "            chosen = int(photo_ranking[0])\n",
    "        assigned_photos.add(chosen)\n",
    "        matches.append((monet_hists[mi][0], photo_hists[chosen][0], float(scores[mi, chosen])))\n",
    "\n",
    "    # sort matches by descending correlation score\n",
    "    matches_sorted = sorted(matches, key=lambda x: -x[2])\n",
    "    return matches_sorted\n",
    "\n",
    "# Run matching for 300 pairs and display the top 10 correlated pairs\n",
    "matches = find_best_unique_matches('/home/ryan-bulger/Monet-GAN/monet_jpg', '/home/ryan-bulger/Monet-GAN/photo_jpg', n_matches=300)\n",
    "print(f'Found {len(matches):,} matched pairs. Showing top 10 by correlation...')\n",
    "\n",
    "# Display top 10 side-by-side\n",
    "top_k = min(10, len(matches))\n",
    "fig, axes = plt.subplots(top_k, 2, figsize=(10, 5 * top_k))\n",
    "for idx in range(top_k):\n",
    "    monet_path, photo_path, score = matches[idx]\n",
    "    m_img = cv2.imread(monet_path)\n",
    "    p_img = cv2.imread(photo_path)\n",
    "    # convert BGR->RGB for matplotlib\n",
    "    if m_img is not None:\n",
    "        m_img = cv2.cvtColor(m_img, cv2.COLOR_BGR2RGB)\n",
    "    if p_img is not None:\n",
    "        p_img = cv2.cvtColor(p_img, cv2.COLOR_BGR2RGB)\n",
    "    axm = axes[idx, 0] if top_k > 1 else axes[0]\n",
    "    axp = axes[idx, 1] if top_k > 1 else axes[1]\n",
    "    if m_img is not None:\n",
    "        axm.imshow(m_img)\n",
    "    axm.set_title(f'Monet: {os.path.basename(monet_path)}')\n",
    "    axm.axis('off')\n",
    "    if p_img is not None:\n",
    "        axp.imshow(p_img)\n",
    "    axp.set_title(f'Photo: {os.path.basename(photo_path)}\\nScore: {score:.4f}')\n",
    "    axp.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the top 10 filenames and scores for reference\n",
    "for i, (m, p, s) in enumerate(matches[:top_k], 1):\n",
    "    print(f'{i:02d}. Monet: {os.path.basename(m)}  <->  Photo: {os.path.basename(p)}  | Score: {s:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686096ce",
   "metadata": {},
   "source": [
    "#### Create Training, Validation, and Test Datasets\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76047e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset labels\n",
    "train_labels_df = pd.read_csv('./train/train_labels.csv')\n",
    "\n",
    "# Take random samples from the train_labels_df (to reduce training time)\n",
    "# train_labels_df = train_labels_df.sample(SAMPLE_SIZE)\n",
    "# test_file_paths = random.sample(test_file_paths, SAMPLE_SIZE)\n",
    "\n",
    "# Training-Validation split\n",
    "train_df, val_df = train_test_split(train_labels_df, test_size=0.2, random_state=42, stratify=train_labels_df['label'])\n",
    "\n",
    "# Create file paths list after train-validation split\n",
    "train_paths = ['./train/' + id + '.tif' for id in train_df['id']]\n",
    "val_paths = ['./train/' + id + '.tif' for id in val_df['id']]\n",
    "\n",
    "# Create label vectors after train-validation split\n",
    "training_labels = train_df['label'].values\n",
    "validation_labels = val_df['label'].values\n",
    "\n",
    "# Numpy function used to read and resize images\n",
    "def _read_and_resize(path):\n",
    "    path = path.decode('utf-8')\n",
    "    img = tiff.imread(path)       \n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "    if img.ndim == 2:\n",
    "        img = np.stack([img]*3, axis=-1)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "#Function to load and preprocess each image\n",
    "def load_image(path, label):\n",
    "    img = tf.numpy_function(_read_and_resize, [path], tf.float32)\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, CHANNELS])  # static shape required by Keras\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    return img, label\n",
    "\n",
    "#Function to load and preprocess each image\n",
    "def load_train_image(path):\n",
    "    img = tf.numpy_function(_read_and_resize, [path], tf.float32)\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, CHANNELS])  # static shape required by Keras\n",
    "    return img\n",
    "\n",
    "# Create training and validation tf.data.Dataset instances\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, training_labels))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, validation_labels))\n",
    "\n",
    "# Create the test set\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_file_paths))\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(buffer_size=len(train_paths))\n",
    "    .map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "print(f'\\nTraining dataset of {len(training_labels):,} images has been created.')\n",
    "\n",
    "# Create validation dataset\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "print(f'\\nValidation dataset of {len(validation_labels):,} images has been created.')\n",
    "\n",
    "# Create validation dataset\n",
    "test_ds = (\n",
    "    test_ds\n",
    "    .map(load_train_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "print(f'\\nTesting dataset of {len(test_file_paths):,} images has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _bytes_feature(value):\n",
    "#     if isinstance(value, type(tf.constant(0))):\n",
    "#         value = value.numpy()\n",
    "#     return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "# def image_to_tfexample(image_string, label):\n",
    "#     feature = {\n",
    "#         'image': _bytes_feature(image_string),\n",
    "#         'label': _bytes_feature(label.encode('utf-8'))\n",
    "#     }\n",
    "#     return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "# def convert_images_to_tfrecords(input_dir, output_dir, label):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "#     for i, filename in enumerate(image_files, 1):\n",
    "#         image_path = os.path.join(input_dir, filename)\n",
    "#         tfrec_name = os.path.splitext(filename)[0] + '.tfrec'\n",
    "#         tfrec_path = os.path.join(output_dir, tfrec_name)\n",
    "\n",
    "#         with open(image_path, 'rb') as f:\n",
    "#             image_bytes = f.read()\n",
    "\n",
    "#         example = image_to_tfexample(image_bytes, label)\n",
    "#         with tf.io.TFRecordWriter(tfrec_path) as writer:\n",
    "#             writer.write(example.SerializeToString())\n",
    "\n",
    "#         if i % 1000 == 0 or i == len(image_files):\n",
    "#             print(f'Processed {i}/{len(image_files)} images into {output_dir}')\n",
    "\n",
    "# # Define folders\n",
    "# photo_jpg_dir = 'photo_jpg'\n",
    "# monet_jpg_dir = 'monet_jpg'\n",
    "# photo_tfrec_dir = 'photo_tfrec'\n",
    "# monet_tfrec_dir = 'monet_tfrec'\n",
    "\n",
    "# # Run conversions\n",
    "# convert_images_to_tfrecords(photo_jpg_dir, photo_tfrec_dir, 'photo')\n",
    "# convert_images_to_tfrecords(monet_jpg_dir, monet_tfrec_dir, 'monet')\n",
    "\n",
    "# print('All .jpg files have been converted to individual .tfrec files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e3f99",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095ca59",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5e602",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ceed9f",
   "metadata": {},
   "source": [
    "## Results & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9b866",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
